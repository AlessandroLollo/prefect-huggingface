"""A simple client to interact with Huggingface Inference APIs."""

import json
from typing import Dict, Optional

from requests.sessions import Session

from prefect_huggingface.exceptions import (
    HuggingfaceAPIFailure,
    HuggingfaceInferenceConfiguration,
)


class HuggingfaceClient:
    """
    Client to interact with Hugginface Inference API.

    Attributes:
        access_token (str): Access token to authenticate with Huggingface Inference API.
    """

    def __init__(self, access_token: str) -> None:
        self.access_token = access_token

    def __get_inference_api_model_url(self, model_id: str) -> str:
        """
        Private method to build the model URL given the model identifier.
        """
        return f"https://api-inference.huggingface.co/models/{model_id}"

    def __get_session(self) -> Session:
        """
        Private method to build a `Session` object to interact with
            Huggingface Inference API.
        """
        session = Session()
        session.headers = {"Authorization": f"Bearer {self.access_token}"}

        return session

    def get_inference_result(
        self,
        inference_endpoint_url: Optional[str],
        model_id: Optional[str],
        inputs: str,
        options: Optional[Dict],
        parameters: Optional[Dict],
    ) -> Dict:
        """
        Returns the inference response generated by Huggingface API.
        Only `inputs` is required, as specified in the [Huggingface API docs.](https://huggingface.co/docs/api-inference/detailed_parameters) # noqa
        `options` and `parameters` maybe be used based on the choosen model.

        Attributes:
            inference_endpoint_url (Optional[str]): The URL of the inference endpoint.
                Should be set when using Inference Endpoints.
            model_id (Optional[str]): The identifier of the model to use.
                Should be set when using Inference API.
            inputs (str): The input string to provide to the model.
            options (Optional[Dict]): Options to pass to the API for the given model.
            parameters (Optional[Dict]): Parameters to pass to the API for the given model.

        Returns:
            The raw response provided by Huggingface API.

        Raises:
            `HuggingfaceInferenceConfiguration` if neither `inference_endpoint_url` nor
                `model_id` are provided.
            `HuggingfaceAPIFailure` if the call to Huggingface API
                returns an error (status != 200).
        """
        if inference_endpoint_url:
            url = inference_endpoint_url
        elif model_id:
            url = self.__get_inference_api_model_url(model_id=model_id)
        else:
            msg = "Please provide either the Inference Endpoint URL or the model identifier to be used with Inference API."  # noqa
            raise HuggingfaceInferenceConfiguration(msg)
        session = self.__get_session()
        data = {"inputs": inputs}
        if options:
            data["options"] = options
        if parameters:
            data["parameters"] = parameters

        with session.post(url=url, data=json.dumps(data)) as response:
            if response.status_code != 200:
                msg = f"There was an error while retrieving result from Huggingface API. Error is: {response.reason}"  # noqa
                raise HuggingfaceAPIFailure(msg)
            else:
                return json.loads(response.content.decode("utf-8"))
