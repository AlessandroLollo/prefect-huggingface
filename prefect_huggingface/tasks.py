"""Tasks to interact with Huggingface Inference API."""

from typing import Dict, Optional

from prefect import task

from prefect_huggingface.credentials import HuggingfaceCredentials


@task
def get_inference_result(
    credentials: HuggingfaceCredentials,
    inference_endpoint_url: Optional[str],
    model_id: Optional[str],
    inputs: str,
    options: Optional[Dict],
    parameters: Optional[Dict],
) -> Dict:
    """
    Returns the inference response generated by Huggingface API.
    Only `inputs` is required, as specified in the [Huggingface API docs.](https://huggingface.co/docs/api-inference/detailed_parameters) # noqa
    `options` and `parameters` maybe be used based on the choosen model.

    Attributes:
        inference_endpoint_url (Optional[str]): The URL of the inference endpoint.
            Should be set when using Inference Endpoints.
        model_id (Optional[str]): The identifier of the model to use.
            Should be set when using Inference API.
        inputs (str): The input string to provide to the model.
        options (Optional[Dict]): Options to pass to the API for the given model.
        parameters (Optional[Dict]): Parameters to pass to the API for the given model.

    Returns:
        The raw response provided by Huggingface API.

    Raises:
        `HuggingfaceInferenceConfiguration` if the call to Huggingface API
            returns an error (status != 200)
    """
    hf_client = credentials.get_client()
    return hf_client.get_inference_result(
        inference_endpoint_url=inference_endpoint_url,
        model_id=model_id,
        inputs=inputs,
        options=options,
        parameters=parameters,
    )
