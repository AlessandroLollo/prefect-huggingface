"""Tasks to interact with Huggingface Inference API."""

from typing import Dict, Optional

from prefect import task

from prefect_huggingface.credentials import HuggingfaceCredentials
from prefect_huggingface.managers.transformers_manager import ModelManager

from transformers.pipelines import Pipeline

@task
def get_inference_result(
    credentials: HuggingfaceCredentials,
    inference_endpoint_url: Optional[str],
    model_id: Optional[str],
    inputs: str,
    options: Optional[Dict],
    parameters: Optional[Dict],
) -> Dict:
    """
    Returns the inference response generated by Huggingface API.
    Only `inputs` is required, as specified in the [Huggingface API docs.](https://huggingface.co/docs/api-inference/detailed_parameters) # noqa
    `options` and `parameters` maybe be used based on the choosen model.

    Attributes:
        inference_endpoint_url (Optional[str]): The URL of the inference endpoint.
            Should be set when using Inference Endpoints.
        model_id (Optional[str]): The identifier of the model to use.
            Should be set when using Inference API.
        inputs (str): The input string to provide to the model.
        options (Optional[Dict]): Options to pass to the API for the given model.
        parameters (Optional[Dict]): Parameters to pass to the API for the given model.

    Returns:
        The raw response provided by Huggingface API.

    Raises:
        `HuggingfaceInferenceConfiguration` if the call to Huggingface API
            returns an error (status != 200)
    """
    hf_client = credentials.get_client()
    return hf_client.get_inference_result(
        inference_endpoint_url=inference_endpoint_url,
        model_id=model_id,
        inputs=inputs,
        options=options,
        parameters=parameters,
    )


@task(name='Load Transformers Pipeline')
def load_transformers_pipeline(transformer_task: str, device: str) -> Pipeline:
    """
        Returns the Transformers pipeline object specific to the specified task

        Args:
            transformer_task (str): The task the user wants to perform.
            device (int): the device where we want to run the inference - either CPU or GPU.
            Users can specify device argument as a str, “cpu” or "cuda" device.

        Returns:
            The HF transformer model

    """
    return ModelManager.get_model(transformer_task=transformer_task, device=device)
